{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Unet with membrane data\n",
    "membrane data is in folder membrane/, it is a binary classification task.\n",
    "\n",
    "The input shape of image and mask are the same :(batch_size,rows,cols,channel = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8496 - accuracy: 0.2129\n",
      "Epoch 00001: loss improved from inf to 0.84959, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.8496 - accuracy: 0.2129\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.8779\n",
      "Epoch 00002: loss improved from 0.84959 to 0.62161, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.6216 - accuracy: 0.8779\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9838 - accuracy: 0.8803\n",
      "Epoch 00003: loss did not improve from 0.62161\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9838 - accuracy: 0.8803\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.8389\n",
      "Epoch 00004: loss did not improve from 0.62161\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6287 - accuracy: 0.8389\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3355 - accuracy: 0.1484\n",
      "Epoch 00005: loss did not improve from 0.62161\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3355 - accuracy: 0.1484\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.7123\n",
      "Epoch 00006: loss improved from 0.62161 to 0.60895, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 0.6090 - accuracy: 0.7123\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8858\n",
      "Epoch 00007: loss improved from 0.60895 to 0.45504, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.4550 - accuracy: 0.8858\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.8877\n",
      "Epoch 00008: loss improved from 0.45504 to 0.43458, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.4346 - accuracy: 0.8877\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8801\n",
      "Epoch 00009: loss did not improve from 0.43458\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8801\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.9062\n",
      "Epoch 00010: loss improved from 0.43458 to 0.33836, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.3384 - accuracy: 0.9062\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9582\n",
      "Epoch 00011: loss improved from 0.33836 to 0.23439, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.2344 - accuracy: 0.9582\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9393\n",
      "Epoch 00012: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9393\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7891 - accuracy: 0.7834\n",
      "Epoch 00013: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7891 - accuracy: 0.7834\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9122\n",
      "Epoch 00014: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3005 - accuracy: 0.9122\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8478\n",
      "Epoch 00015: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8478\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8859\n",
      "Epoch 00016: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3854 - accuracy: 0.8859\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.9002\n",
      "Epoch 00017: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.9002\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.9133\n",
      "Epoch 00018: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.9133\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.8244\n",
      "Epoch 00019: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.8244\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4687 - accuracy: 0.8364\n",
      "Epoch 00020: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.8364\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.9011\n",
      "Epoch 00021: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3220 - accuracy: 0.9011\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9326\n",
      "Epoch 00022: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.9326\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8697\n",
      "Epoch 00023: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3916 - accuracy: 0.8697\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.8856\n",
      "Epoch 00024: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8856\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8850\n",
      "Epoch 00025: loss did not improve from 0.23439\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8850\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.9588\n",
      "Epoch 00026: loss improved from 0.23439 to 0.18628, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.1863 - accuracy: 0.9588\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9389\n",
      "Epoch 00027: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9389\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9473\n",
      "Epoch 00028: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9473\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.9061\n",
      "Epoch 00029: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.9061\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.7836\n",
      "Epoch 00030: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6976 - accuracy: 0.7836\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8890\n",
      "Epoch 00031: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8890\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8936\n",
      "Epoch 00032: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8936\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9333\n",
      "Epoch 00033: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2855 - accuracy: 0.9333\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9493\n",
      "Epoch 00034: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.9493\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.7291\n",
      "Epoch 00035: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6356 - accuracy: 0.7291\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9325\n",
      "Epoch 00036: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.9325\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9218\n",
      "Epoch 00037: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.9218\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9536\n",
      "Epoch 00038: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9536\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.8159\n",
      "Epoch 00039: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.5093 - accuracy: 0.8159\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.8778\n",
      "Epoch 00040: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8778\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9375\n",
      "Epoch 00041: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9375\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8426\n",
      "Epoch 00042: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.8426\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9041\n",
      "Epoch 00043: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.9041\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9458\n",
      "Epoch 00044: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2260 - accuracy: 0.9458\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9217\n",
      "Epoch 00045: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.9217\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8853\n",
      "Epoch 00046: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.8853\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.8150\n",
      "Epoch 00047: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5491 - accuracy: 0.8150\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9409\n",
      "Epoch 00048: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9409\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9260\n",
      "Epoch 00049: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2801 - accuracy: 0.9260\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.7880\n",
      "Epoch 00050: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7880\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8767\n",
      "Epoch 00051: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3612 - accuracy: 0.8767\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9306\n",
      "Epoch 00052: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.9306\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8804\n",
      "Epoch 00053: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3603 - accuracy: 0.8804\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9315\n",
      "Epoch 00054: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2721 - accuracy: 0.9315\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9153\n",
      "Epoch 00055: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2768 - accuracy: 0.9153\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9428\n",
      "Epoch 00056: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9428\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.8618\n",
      "Epoch 00057: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4179 - accuracy: 0.8618\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9230\n",
      "Epoch 00058: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9230\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.8732\n",
      "Epoch 00059: loss did not improve from 0.18628\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3619 - accuracy: 0.8732\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9609\n",
      "Epoch 00060: loss improved from 0.18628 to 0.17996, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.1800 - accuracy: 0.9609\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9167\n",
      "Epoch 00061: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9167\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8996\n",
      "Epoch 00062: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3214 - accuracy: 0.8996\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8006\n",
      "Epoch 00063: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.8006\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.8091\n",
      "Epoch 00064: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5102 - accuracy: 0.8091\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8844\n",
      "Epoch 00065: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8844\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9160\n",
      "Epoch 00066: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.9160\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.9086\n",
      "Epoch 00067: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.9086\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9556\n",
      "Epoch 00068: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.9556\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9443\n",
      "Epoch 00069: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.9443\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.9158\n",
      "Epoch 00070: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.9158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9539\n",
      "Epoch 00071: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9539\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8874\n",
      "Epoch 00072: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8874\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.8979\n",
      "Epoch 00073: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2830 - accuracy: 0.8979\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9453\n",
      "Epoch 00074: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9453\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4571 - accuracy: 0.8251\n",
      "Epoch 00075: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.8251\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.8065\n",
      "Epoch 00076: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5648 - accuracy: 0.8065\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9146\n",
      "Epoch 00077: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.9146\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9225\n",
      "Epoch 00078: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.9225\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9391\n",
      "Epoch 00079: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.9391\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4152 - accuracy: 0.8373\n",
      "Epoch 00080: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4152 - accuracy: 0.8373\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.9031\n",
      "Epoch 00081: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2927 - accuracy: 0.9031\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8472\n",
      "Epoch 00082: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8472\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9487\n",
      "Epoch 00083: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2352 - accuracy: 0.9487\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.8929\n",
      "Epoch 00084: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8929\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8868\n",
      "Epoch 00085: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.8868\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9237\n",
      "Epoch 00086: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9237\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9277\n",
      "Epoch 00087: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9277\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8928\n",
      "Epoch 00088: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3639 - accuracy: 0.8928\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8977\n",
      "Epoch 00089: loss did not improve from 0.17996\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2703 - accuracy: 0.8977\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9603\n",
      "Epoch 00090: loss improved from 0.17996 to 0.17077, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.1708 - accuracy: 0.9603\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9015\n",
      "Epoch 00091: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.9015\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9376\n",
      "Epoch 00092: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9376\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4433 - accuracy: 0.8377\n",
      "Epoch 00093: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4433 - accuracy: 0.8377\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8278\n",
      "Epoch 00094: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8278\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.8642\n",
      "Epoch 00095: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8642\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.9131\n",
      "Epoch 00096: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.9131\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8926\n",
      "Epoch 00097: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2913 - accuracy: 0.8926\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9281\n",
      "Epoch 00098: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.9281\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.9502\n",
      "Epoch 00099: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2420 - accuracy: 0.9502\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.8931\n",
      "Epoch 00100: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3027 - accuracy: 0.8931\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9033\n",
      "Epoch 00101: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.9033\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9470\n",
      "Epoch 00102: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9470\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8878\n",
      "Epoch 00103: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8878\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8943\n",
      "Epoch 00104: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3464 - accuracy: 0.8943\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.8741\n",
      "Epoch 00105: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8741\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9426\n",
      "Epoch 00106: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9163\n",
      "Epoch 00107: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2448 - accuracy: 0.9163\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8678\n",
      "Epoch 00108: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8678\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8977\n",
      "Epoch 00109: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3197 - accuracy: 0.8977\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9082\n",
      "Epoch 00110: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.9082\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.8058\n",
      "Epoch 00111: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4723 - accuracy: 0.8058\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9359\n",
      "Epoch 00112: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9359\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.8615\n",
      "Epoch 00113: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8615\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9633\n",
      "Epoch 00114: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9633\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9151\n",
      "Epoch 00115: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2385 - accuracy: 0.9151\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9256\n",
      "Epoch 00116: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2502 - accuracy: 0.9256\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8970\n",
      "Epoch 00117: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8970\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8928\n",
      "Epoch 00118: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3384 - accuracy: 0.8928\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8492\n",
      "Epoch 00119: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3970 - accuracy: 0.8492\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9079\n",
      "Epoch 00120: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.9079\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.8944\n",
      "Epoch 00121: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3001 - accuracy: 0.8944\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8925\n",
      "Epoch 00122: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8925\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9415\n",
      "Epoch 00123: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2401 - accuracy: 0.9415\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9212\n",
      "Epoch 00124: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.9212\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9518\n",
      "Epoch 00125: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9518\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8843\n",
      "Epoch 00126: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8843\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.8888\n",
      "Epoch 00127: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8888\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.8920\n",
      "Epoch 00128: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2734 - accuracy: 0.8920\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.8397\n",
      "Epoch 00129: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3977 - accuracy: 0.8397\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7592 - accuracy: 0.7328\n",
      "Epoch 00130: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7592 - accuracy: 0.7328\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9157\n",
      "Epoch 00131: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.9157\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9623\n",
      "Epoch 00132: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9623\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9501\n",
      "Epoch 00133: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.9501\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.8747\n",
      "Epoch 00134: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8747\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9139\n",
      "Epoch 00135: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3021 - accuracy: 0.9139\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8130\n",
      "Epoch 00136: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4386 - accuracy: 0.8130\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9186\n",
      "Epoch 00137: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.9186\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.8603\n",
      "Epoch 00138: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8603\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9211\n",
      "Epoch 00139: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.9211\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.8816\n",
      "Epoch 00140: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.8816\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9084\n",
      "Epoch 00141: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2274 - accuracy: 0.9084\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9644\n",
      "Epoch 00142: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.9480\n",
      "Epoch 00143: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9480\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.8736\n",
      "Epoch 00144: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.8736\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9155\n",
      "Epoch 00145: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2461 - accuracy: 0.9155\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.8904\n",
      "Epoch 00146: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8904\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.8978\n",
      "Epoch 00147: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8978\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.8893\n",
      "Epoch 00148: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3279 - accuracy: 0.8893\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9398\n",
      "Epoch 00149: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.9398\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.8206\n",
      "Epoch 00150: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.8206\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.8620\n",
      "Epoch 00151: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8620\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9507\n",
      "Epoch 00152: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9507\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9229\n",
      "Epoch 00153: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.9229\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.9208\n",
      "Epoch 00154: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.9208\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.8157\n",
      "Epoch 00155: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8157\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.8755\n",
      "Epoch 00156: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3173 - accuracy: 0.8755\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9234\n",
      "Epoch 00157: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.9234\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9592\n",
      "Epoch 00158: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9592\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8727\n",
      "Epoch 00159: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8727\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8179\n",
      "Epoch 00160: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4197 - accuracy: 0.8179\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9454\n",
      "Epoch 00161: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9454\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9249\n",
      "Epoch 00162: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2102 - accuracy: 0.9249\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.8291\n",
      "Epoch 00163: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8291\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9364\n",
      "Epoch 00164: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9364\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9293\n",
      "Epoch 00165: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2249 - accuracy: 0.9293\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8559\n",
      "Epoch 00166: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8559\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9588\n",
      "Epoch 00167: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2011 - accuracy: 0.9588\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9456\n",
      "Epoch 00168: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9456\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9018\n",
      "Epoch 00169: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9018\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8821\n",
      "Epoch 00170: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8821\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9429\n",
      "Epoch 00171: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9429\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.7922\n",
      "Epoch 00172: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.7922\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9234\n",
      "Epoch 00173: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2190 - accuracy: 0.9234\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.8966\n",
      "Epoch 00174: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.8966\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3930 - accuracy: 0.8304\n",
      "Epoch 00175: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8304\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8479\n",
      "Epoch 00176: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8479\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.9314\n",
      "Epoch 00177: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2802 - accuracy: 0.9314\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9600\n",
      "Epoch 00178: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8935\n",
      "Epoch 00179: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3149 - accuracy: 0.8935\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9098\n",
      "Epoch 00180: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3194 - accuracy: 0.9098\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9601\n",
      "Epoch 00181: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9601\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8499\n",
      "Epoch 00182: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8499\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9214\n",
      "Epoch 00183: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2383 - accuracy: 0.9214\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9369\n",
      "Epoch 00184: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9369\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8126\n",
      "Epoch 00185: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4967 - accuracy: 0.8126\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3372 - accuracy: 0.8906\n",
      "Epoch 00186: loss did not improve from 0.17077\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3372 - accuracy: 0.8906\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9660\n",
      "Epoch 00187: loss improved from 0.17077 to 0.14056, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.1406 - accuracy: 0.9660\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.8273\n",
      "Epoch 00188: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8273\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9370\n",
      "Epoch 00189: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2263 - accuracy: 0.9370\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9037\n",
      "Epoch 00190: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9037\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.8565\n",
      "Epoch 00191: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3862 - accuracy: 0.8565\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8985\n",
      "Epoch 00192: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2928 - accuracy: 0.8985\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8744\n",
      "Epoch 00193: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2943 - accuracy: 0.8744\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9318\n",
      "Epoch 00194: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2542 - accuracy: 0.9318\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9156\n",
      "Epoch 00195: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2856 - accuracy: 0.9156\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.8892\n",
      "Epoch 00196: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2404 - accuracy: 0.8892\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9507\n",
      "Epoch 00197: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9507\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9252\n",
      "Epoch 00198: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.9252\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9631\n",
      "Epoch 00199: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1839 - accuracy: 0.9631\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9496\n",
      "Epoch 00200: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9496\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.8035\n",
      "Epoch 00201: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5681 - accuracy: 0.8035\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.8008\n",
      "Epoch 00202: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.8008\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8388\n",
      "Epoch 00203: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8388\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9194\n",
      "Epoch 00204: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.9194\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.9416\n",
      "Epoch 00205: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.9416\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.9015\n",
      "Epoch 00206: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3243 - accuracy: 0.9015\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9300\n",
      "Epoch 00207: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.9300\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8716\n",
      "Epoch 00208: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8716\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9026\n",
      "Epoch 00209: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2904 - accuracy: 0.9026\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8804\n",
      "Epoch 00210: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3143 - accuracy: 0.8804\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.8961\n",
      "Epoch 00211: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.8961\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9440\n",
      "Epoch 00212: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9440\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9515\n",
      "Epoch 00213: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9515\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.8441\n",
      "Epoch 00214: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.8805\n",
      "Epoch 00215: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8805\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9434\n",
      "Epoch 00216: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9434\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.9179\n",
      "Epoch 00217: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.9179\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.8297\n",
      "Epoch 00218: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.8297\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9232\n",
      "Epoch 00219: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.9232\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9301\n",
      "Epoch 00220: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9301\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7992\n",
      "Epoch 00221: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7992\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9640\n",
      "Epoch 00222: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.9640\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.7909\n",
      "Epoch 00223: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7909\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9192\n",
      "Epoch 00224: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.9192\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9435\n",
      "Epoch 00225: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.9435\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9113\n",
      "Epoch 00226: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2861 - accuracy: 0.9113\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.8074\n",
      "Epoch 00227: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4893 - accuracy: 0.8074\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.8749\n",
      "Epoch 00228: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8749\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9649\n",
      "Epoch 00229: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9649\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9245\n",
      "Epoch 00230: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2366 - accuracy: 0.9245\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9202\n",
      "Epoch 00231: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2086 - accuracy: 0.9202\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8761\n",
      "Epoch 00232: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8761\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9328\n",
      "Epoch 00233: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9328\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9587\n",
      "Epoch 00234: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9587\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8126\n",
      "Epoch 00235: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.8126\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3932 - accuracy: 0.8745\n",
      "Epoch 00236: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3932 - accuracy: 0.8745\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9581\n",
      "Epoch 00237: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9581\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9345\n",
      "Epoch 00238: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9345\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.8335\n",
      "Epoch 00239: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3934 - accuracy: 0.8335\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8952\n",
      "Epoch 00240: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3047 - accuracy: 0.8952\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9413\n",
      "Epoch 00241: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9413\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.8779\n",
      "Epoch 00242: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.8779\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9099\n",
      "Epoch 00243: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2696 - accuracy: 0.9099\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8933\n",
      "Epoch 00244: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2760 - accuracy: 0.8933\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9323\n",
      "Epoch 00245: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.9323\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.8546\n",
      "Epoch 00246: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8546\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8665\n",
      "Epoch 00247: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8665\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9229\n",
      "Epoch 00248: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1983 - accuracy: 0.9229\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.8291\n",
      "Epoch 00249: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4123 - accuracy: 0.8291\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9457\n",
      "Epoch 00250: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9579\n",
      "Epoch 00251: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9579\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.7925\n",
      "Epoch 00252: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7925\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9244\n",
      "Epoch 00253: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9244\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8586\n",
      "Epoch 00254: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3594 - accuracy: 0.8586\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9550\n",
      "Epoch 00255: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9550\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.8764\n",
      "Epoch 00256: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8764\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8477\n",
      "Epoch 00257: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3899 - accuracy: 0.8477\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8848\n",
      "Epoch 00258: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8848\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8483\n",
      "Epoch 00259: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8483\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9322\n",
      "Epoch 00260: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9322\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.7668\n",
      "Epoch 00261: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.7668\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9400\n",
      "Epoch 00262: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.9400\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8803\n",
      "Epoch 00263: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8803\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9184\n",
      "Epoch 00264: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9184\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9053\n",
      "Epoch 00265: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9053\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9441\n",
      "Epoch 00266: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9441\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9420\n",
      "Epoch 00267: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9420\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9352\n",
      "Epoch 00268: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9352\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.8968\n",
      "Epoch 00269: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.8968\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9506\n",
      "Epoch 00270: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9506\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9012\n",
      "Epoch 00271: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.9012\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9570\n",
      "Epoch 00272: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9570\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.8087\n",
      "Epoch 00273: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4870 - accuracy: 0.8087\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9100\n",
      "Epoch 00274: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.9100\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9403\n",
      "Epoch 00275: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2448 - accuracy: 0.9403\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8725\n",
      "Epoch 00276: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8725\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.8706\n",
      "Epoch 00277: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2892 - accuracy: 0.8706\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8821\n",
      "Epoch 00278: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8821\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8716\n",
      "Epoch 00279: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3306 - accuracy: 0.8716\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9031\n",
      "Epoch 00280: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.9031\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.9053\n",
      "Epoch 00281: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.9053\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9550\n",
      "Epoch 00282: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2405 - accuracy: 0.9550\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.8916\n",
      "Epoch 00283: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8916\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9654\n",
      "Epoch 00284: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9654\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8316\n",
      "Epoch 00285: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4244 - accuracy: 0.8316\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8810\n",
      "Epoch 00286: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9528\n",
      "Epoch 00287: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9528\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 0.8523\n",
      "Epoch 00288: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3408 - accuracy: 0.8523\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8590\n",
      "Epoch 00289: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.8590\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.7827\n",
      "Epoch 00290: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.7827\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8867\n",
      "Epoch 00291: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2833 - accuracy: 0.8867\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9162\n",
      "Epoch 00292: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9162\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.8931\n",
      "Epoch 00293: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2641 - accuracy: 0.8931\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9220\n",
      "Epoch 00294: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9220\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9645\n",
      "Epoch 00295: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1722 - accuracy: 0.9645\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.8927\n",
      "Epoch 00296: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8927\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9418\n",
      "Epoch 00297: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9418\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.8285\n",
      "Epoch 00298: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3983 - accuracy: 0.8285\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9381\n",
      "Epoch 00299: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9381\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9391\n",
      "Epoch 00300: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9391\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8953\n",
      "Epoch 00301: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8953\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8917\n",
      "Epoch 00302: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3476 - accuracy: 0.8917\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.8367\n",
      "Epoch 00303: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.8367\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9623\n",
      "Epoch 00304: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9623\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9142\n",
      "Epoch 00305: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9142\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9353\n",
      "Epoch 00306: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1909 - accuracy: 0.9353\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9094\n",
      "Epoch 00307: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.9094\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8912\n",
      "Epoch 00308: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8912\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8082\n",
      "Epoch 00309: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8082\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.8877\n",
      "Epoch 00310: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2717 - accuracy: 0.8877\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9673\n",
      "Epoch 00311: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9673\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.9290\n",
      "Epoch 00312: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9290\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8278\n",
      "Epoch 00313: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8278\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8866\n",
      "Epoch 00314: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8866\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9382\n",
      "Epoch 00315: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9382\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.8852\n",
      "Epoch 00316: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3109 - accuracy: 0.8852\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8348\n",
      "Epoch 00317: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.8348\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9197\n",
      "Epoch 00318: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9197\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9278\n",
      "Epoch 00319: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2680 - accuracy: 0.9278\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8809\n",
      "Epoch 00320: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8809\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.8962\n",
      "Epoch 00321: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.8962\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8141\n",
      "Epoch 00322: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4883 - accuracy: 0.8141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9345\n",
      "Epoch 00323: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2633 - accuracy: 0.9345\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9272\n",
      "Epoch 00324: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9272\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9281\n",
      "Epoch 00325: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.9281\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.8073\n",
      "Epoch 00326: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.8073\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.8770\n",
      "Epoch 00327: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2528 - accuracy: 0.8770\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9359\n",
      "Epoch 00328: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2291 - accuracy: 0.9359\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9418\n",
      "Epoch 00329: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9418\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9627\n",
      "Epoch 00330: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9627\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9451\n",
      "Epoch 00331: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9451\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.8893\n",
      "Epoch 00332: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2531 - accuracy: 0.8893\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.8227\n",
      "Epoch 00333: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4970 - accuracy: 0.8227\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.9016\n",
      "Epoch 00334: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3335 - accuracy: 0.9016\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.8621\n",
      "Epoch 00335: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3597 - accuracy: 0.8621\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9262\n",
      "Epoch 00336: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9262\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9605\n",
      "Epoch 00337: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9605\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.8126\n",
      "Epoch 00338: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8126\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9018\n",
      "Epoch 00339: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2852 - accuracy: 0.9018\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.8749\n",
      "Epoch 00340: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2895 - accuracy: 0.8749\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9214\n",
      "Epoch 00341: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.9214\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.8977\n",
      "Epoch 00342: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2790 - accuracy: 0.8977\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9189\n",
      "Epoch 00343: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2213 - accuracy: 0.9189\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9495\n",
      "Epoch 00344: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9495\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.8715\n",
      "Epoch 00345: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3666 - accuracy: 0.8715\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9223\n",
      "Epoch 00346: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.9223\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8510\n",
      "Epoch 00347: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8510\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9413\n",
      "Epoch 00348: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1780 - accuracy: 0.9413\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.8755\n",
      "Epoch 00349: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2740 - accuracy: 0.8755\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8604\n",
      "Epoch 00350: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8604\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.8760\n",
      "Epoch 00351: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8760\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7620\n",
      "Epoch 00352: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5683 - accuracy: 0.7620\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9489\n",
      "Epoch 00353: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9489\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9419\n",
      "Epoch 00354: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2074 - accuracy: 0.9419\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9591\n",
      "Epoch 00355: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9591\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9492\n",
      "Epoch 00356: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.9492\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.8931\n",
      "Epoch 00357: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8931\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9039\n",
      "Epoch 00358: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.9039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9102\n",
      "Epoch 00359: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9102\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8714\n",
      "Epoch 00360: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3079 - accuracy: 0.8714\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9660\n",
      "Epoch 00361: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1560 - accuracy: 0.9660\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9456\n",
      "Epoch 00362: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9456\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8349\n",
      "Epoch 00363: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8349\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.8998\n",
      "Epoch 00364: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8998\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.8968\n",
      "Epoch 00365: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.8968\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9053\n",
      "Epoch 00366: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.9053\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.8251\n",
      "Epoch 00367: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.8251\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9587\n",
      "Epoch 00368: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9587\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9075\n",
      "Epoch 00369: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2317 - accuracy: 0.9075\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.8524\n",
      "Epoch 00370: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8524\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4544 - accuracy: 0.7870\n",
      "Epoch 00371: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4544 - accuracy: 0.7870\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8907\n",
      "Epoch 00372: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8907\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9600\n",
      "Epoch 00373: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9600\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9415\n",
      "Epoch 00374: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9415\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9153\n",
      "Epoch 00375: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9153\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9087\n",
      "Epoch 00376: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2491 - accuracy: 0.9087\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9627\n",
      "Epoch 00377: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9627\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.8976\n",
      "Epoch 00378: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8976\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8387\n",
      "Epoch 00379: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8387\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9236\n",
      "Epoch 00380: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2506 - accuracy: 0.9236\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9591\n",
      "Epoch 00381: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9591\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2987 - accuracy: 0.9179\n",
      "Epoch 00382: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.9179\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.8143\n",
      "Epoch 00383: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4927 - accuracy: 0.8143\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3725 - accuracy: 0.8430\n",
      "Epoch 00384: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3725 - accuracy: 0.8430\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.8572\n",
      "Epoch 00385: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8572\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9303\n",
      "Epoch 00386: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9303\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9394\n",
      "Epoch 00387: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.9394\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9231\n",
      "Epoch 00388: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.9231\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.8149\n",
      "Epoch 00389: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.8149\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9313\n",
      "Epoch 00390: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2676 - accuracy: 0.9313\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.8698\n",
      "Epoch 00391: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8698\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.8993\n",
      "Epoch 00392: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.8993\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9100\n",
      "Epoch 00393: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2379 - accuracy: 0.9100\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.8631\n",
      "Epoch 00394: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9418\n",
      "Epoch 00395: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9418\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.8986\n",
      "Epoch 00396: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8986\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9322\n",
      "Epoch 00397: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9322\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8825\n",
      "Epoch 00398: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8825\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9494\n",
      "Epoch 00399: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9494\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.9107\n",
      "Epoch 00400: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.9107\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.8153\n",
      "Epoch 00401: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5902 - accuracy: 0.8153\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9583\n",
      "Epoch 00402: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9583\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8561\n",
      "Epoch 00403: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8561\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.9028\n",
      "Epoch 00404: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2951 - accuracy: 0.9028\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8973\n",
      "Epoch 00405: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3012 - accuracy: 0.8973\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9390\n",
      "Epoch 00406: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.9390\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9503\n",
      "Epoch 00407: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9503\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8961\n",
      "Epoch 00408: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8961\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8612\n",
      "Epoch 00409: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3577 - accuracy: 0.8612\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.8656\n",
      "Epoch 00410: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8656\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9656\n",
      "Epoch 00411: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9656\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9321\n",
      "Epoch 00412: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9321\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9423\n",
      "Epoch 00413: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9423\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9051\n",
      "Epoch 00414: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2477 - accuracy: 0.9051\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7778\n",
      "Epoch 00415: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5460 - accuracy: 0.7778\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9040\n",
      "Epoch 00416: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9040\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4090 - accuracy: 0.8318\n",
      "Epoch 00417: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.8318\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8520\n",
      "Epoch 00418: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3561 - accuracy: 0.8520\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9220\n",
      "Epoch 00419: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.9220\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9208\n",
      "Epoch 00420: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2864 - accuracy: 0.9208\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.8755\n",
      "Epoch 00421: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8755\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9530\n",
      "Epoch 00422: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9530\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9558\n",
      "Epoch 00423: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9558\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.8528\n",
      "Epoch 00424: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3591 - accuracy: 0.8528\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7467\n",
      "Epoch 00425: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.7467\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.9091\n",
      "Epoch 00426: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3146 - accuracy: 0.9091\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9112\n",
      "Epoch 00427: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.9112\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9025\n",
      "Epoch 00428: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.9025\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4077 - accuracy: 0.8243\n",
      "Epoch 00429: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4077 - accuracy: 0.8243\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9197\n",
      "Epoch 00430: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9136\n",
      "Epoch 00431: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9136\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.9023\n",
      "Epoch 00432: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3489 - accuracy: 0.9023\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.9177\n",
      "Epoch 00433: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.9177\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9672\n",
      "Epoch 00434: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9672\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9329\n",
      "Epoch 00435: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.9329\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9357\n",
      "Epoch 00436: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9357\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.8706\n",
      "Epoch 00437: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8706\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8725\n",
      "Epoch 00438: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3233 - accuracy: 0.8725\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8505\n",
      "Epoch 00439: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8505\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9460\n",
      "Epoch 00440: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9460\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9228\n",
      "Epoch 00441: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9228\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8218\n",
      "Epoch 00442: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3835 - accuracy: 0.8218\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9242\n",
      "Epoch 00443: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2495 - accuracy: 0.9242\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9265\n",
      "Epoch 00444: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.9265\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9643\n",
      "Epoch 00445: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9643\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9330\n",
      "Epoch 00446: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9330\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4234 - accuracy: 0.8091\n",
      "Epoch 00447: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8091\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.8399\n",
      "Epoch 00448: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4393 - accuracy: 0.8399\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9469\n",
      "Epoch 00449: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2157 - accuracy: 0.9469\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9253\n",
      "Epoch 00450: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.9253\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9061\n",
      "Epoch 00451: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2405 - accuracy: 0.9061\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9451\n",
      "Epoch 00452: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2270 - accuracy: 0.9451\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8422\n",
      "Epoch 00453: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8422\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8994\n",
      "Epoch 00454: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8994\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9349\n",
      "Epoch 00455: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2272 - accuracy: 0.9349\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9008\n",
      "Epoch 00456: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.9008\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.8290\n",
      "Epoch 00457: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4153 - accuracy: 0.8290\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8598\n",
      "Epoch 00458: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8598\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.8914\n",
      "Epoch 00459: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8914\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8998\n",
      "Epoch 00460: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2883 - accuracy: 0.8998\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.8861\n",
      "Epoch 00461: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2745 - accuracy: 0.8861\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4218 - accuracy: 0.8095\n",
      "Epoch 00462: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8095\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9564\n",
      "Epoch 00463: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9564\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9265\n",
      "Epoch 00464: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9265\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9587\n",
      "Epoch 00465: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9587\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9456\n",
      "Epoch 00466: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 467/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9048\n",
      "Epoch 00467: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.9048\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9672\n",
      "Epoch 00468: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9672\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9612\n",
      "Epoch 00469: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9612\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.8522\n",
      "Epoch 00470: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.8522\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8889\n",
      "Epoch 00471: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8889\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.8224\n",
      "Epoch 00472: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.8224\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8166\n",
      "Epoch 00473: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4102 - accuracy: 0.8166\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9378\n",
      "Epoch 00474: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.9378\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.8976\n",
      "Epoch 00475: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2818 - accuracy: 0.8976\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.8984\n",
      "Epoch 00476: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.8984\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8820\n",
      "Epoch 00477: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3117 - accuracy: 0.8820\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9177\n",
      "Epoch 00478: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.9177\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9328\n",
      "Epoch 00479: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.9328\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.8539\n",
      "Epoch 00480: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8539\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9360\n",
      "Epoch 00481: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 0.9360\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9170\n",
      "Epoch 00482: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9170\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8837\n",
      "Epoch 00483: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8837\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9131\n",
      "Epoch 00484: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9131\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9041\n",
      "Epoch 00485: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.9041\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9553\n",
      "Epoch 00486: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1935 - accuracy: 0.9553\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9427\n",
      "Epoch 00487: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2365 - accuracy: 0.9427\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9158\n",
      "Epoch 00488: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 0.9158\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8584\n",
      "Epoch 00489: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8584\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9213\n",
      "Epoch 00490: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2628 - accuracy: 0.9213\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.7824\n",
      "Epoch 00491: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.7824\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8538\n",
      "Epoch 00492: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8538\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9310\n",
      "Epoch 00493: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.9310\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9381\n",
      "Epoch 00494: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9381\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8178\n",
      "Epoch 00495: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8178\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8919\n",
      "Epoch 00496: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.8919\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8492\n",
      "Epoch 00497: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8492\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9387\n",
      "Epoch 00498: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9387\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8826\n",
      "Epoch 00499: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3141 - accuracy: 0.8826\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9081\n",
      "Epoch 00500: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9081\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8004\n",
      "Epoch 00501: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8004\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4104 - accuracy: 0.8034\n",
      "Epoch 00502: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4104 - accuracy: 0.8034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 503/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9218\n",
      "Epoch 00503: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2355 - accuracy: 0.9218\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9665\n",
      "Epoch 00504: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9665\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.9211\n",
      "Epoch 00505: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9211\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9383\n",
      "Epoch 00506: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9383\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9436\n",
      "Epoch 00507: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.9436\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9506\n",
      "Epoch 00508: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9506\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.8559\n",
      "Epoch 00509: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8559\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8820\n",
      "Epoch 00510: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8820\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8394\n",
      "Epoch 00511: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4162 - accuracy: 0.8394\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.8461\n",
      "Epoch 00512: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8461\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8641\n",
      "Epoch 00513: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8641\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9636\n",
      "Epoch 00514: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2077 - accuracy: 0.9636\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.8971\n",
      "Epoch 00515: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8971\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9320\n",
      "Epoch 00516: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.9320\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8507\n",
      "Epoch 00517: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8507\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8792\n",
      "Epoch 00518: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3441 - accuracy: 0.8792\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9249\n",
      "Epoch 00519: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2665 - accuracy: 0.9249\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9125\n",
      "Epoch 00520: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2557 - accuracy: 0.9125\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9430\n",
      "Epoch 00521: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9430\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.9199\n",
      "Epoch 00522: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2644 - accuracy: 0.9199\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9055\n",
      "Epoch 00523: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9055\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4115 - accuracy: 0.8557\n",
      "Epoch 00524: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4115 - accuracy: 0.8557\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9324\n",
      "Epoch 00525: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.9324\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9090\n",
      "Epoch 00526: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.9090\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9127\n",
      "Epoch 00527: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9127\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9158\n",
      "Epoch 00528: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9158\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.8801\n",
      "Epoch 00529: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8801\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.8572\n",
      "Epoch 00530: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.8572\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9260\n",
      "Epoch 00531: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2330 - accuracy: 0.9260\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8443\n",
      "Epoch 00532: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8443\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9562\n",
      "Epoch 00533: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1720 - accuracy: 0.9562\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.8856\n",
      "Epoch 00534: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8856\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9017\n",
      "Epoch 00535: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9017\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8403\n",
      "Epoch 00536: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3869 - accuracy: 0.8403\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9288\n",
      "Epoch 00537: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.9288\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9417\n",
      "Epoch 00538: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2254 - accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 539/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9351\n",
      "Epoch 00539: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9351\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3706 - accuracy: 0.8440\n",
      "Epoch 00540: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8440\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8210\n",
      "Epoch 00541: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8210\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9132\n",
      "Epoch 00542: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.9132\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9325\n",
      "Epoch 00543: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.9325\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.9213\n",
      "Epoch 00544: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2428 - accuracy: 0.9213\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.8974\n",
      "Epoch 00545: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.8974\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.8694\n",
      "Epoch 00546: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3151 - accuracy: 0.8694\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9367\n",
      "Epoch 00547: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9367\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8840\n",
      "Epoch 00548: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.8840\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9100\n",
      "Epoch 00549: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2763 - accuracy: 0.9100\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4748 - accuracy: 0.8148\n",
      "Epoch 00550: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.8148\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9195\n",
      "Epoch 00551: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1913 - accuracy: 0.9195\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9521\n",
      "Epoch 00552: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1917 - accuracy: 0.9521\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9306\n",
      "Epoch 00553: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9306\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8534\n",
      "Epoch 00554: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8534\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9136\n",
      "Epoch 00555: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2228 - accuracy: 0.9136\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9518\n",
      "Epoch 00556: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9518\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.8913\n",
      "Epoch 00557: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2410 - accuracy: 0.8913\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9283\n",
      "Epoch 00558: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.9283\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8522\n",
      "Epoch 00559: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3421 - accuracy: 0.8522\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8552\n",
      "Epoch 00560: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3718 - accuracy: 0.8552\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7709\n",
      "Epoch 00561: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7709\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8049\n",
      "Epoch 00562: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3951 - accuracy: 0.8049\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.8936\n",
      "Epoch 00563: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.8936\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8679\n",
      "Epoch 00564: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3697 - accuracy: 0.8679\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9519\n",
      "Epoch 00565: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2541 - accuracy: 0.9519\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9314\n",
      "Epoch 00566: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2182 - accuracy: 0.9314\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9468\n",
      "Epoch 00567: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9468\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9301\n",
      "Epoch 00568: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9301\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.9246\n",
      "Epoch 00569: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9246\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9432\n",
      "Epoch 00570: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2267 - accuracy: 0.9432\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.9032\n",
      "Epoch 00571: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.9032\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9244\n",
      "Epoch 00572: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.9244\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9140\n",
      "Epoch 00573: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.9140\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9399\n",
      "Epoch 00574: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 575/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8996\n",
      "Epoch 00575: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3126 - accuracy: 0.8996\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9487\n",
      "Epoch 00576: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9487\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9440\n",
      "Epoch 00577: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9440\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.8528\n",
      "Epoch 00578: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3492 - accuracy: 0.8528\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.7594\n",
      "Epoch 00579: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5675 - accuracy: 0.7594\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9217\n",
      "Epoch 00580: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9217\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.8746\n",
      "Epoch 00581: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8746\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9143\n",
      "Epoch 00582: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.9143\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9504\n",
      "Epoch 00583: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2151 - accuracy: 0.9504\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.8886\n",
      "Epoch 00584: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3174 - accuracy: 0.8886\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8294\n",
      "Epoch 00585: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8294\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8788\n",
      "Epoch 00586: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8788\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9384\n",
      "Epoch 00587: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9384\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9555\n",
      "Epoch 00588: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1946 - accuracy: 0.9555\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9369\n",
      "Epoch 00589: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9369\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8530\n",
      "Epoch 00590: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3832 - accuracy: 0.8530\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.8809\n",
      "Epoch 00591: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3596 - accuracy: 0.8809\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9268\n",
      "Epoch 00592: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2580 - accuracy: 0.9268\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.8450\n",
      "Epoch 00593: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8450\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9093\n",
      "Epoch 00594: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9093\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9189\n",
      "Epoch 00595: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9189\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.8991\n",
      "Epoch 00596: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.8991\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.7975\n",
      "Epoch 00597: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4704 - accuracy: 0.7975\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9391\n",
      "Epoch 00598: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9391\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.8641\n",
      "Epoch 00599: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8641\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9247\n",
      "Epoch 00600: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2555 - accuracy: 0.9247\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9076\n",
      "Epoch 00601: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9076\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9026\n",
      "Epoch 00602: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2279 - accuracy: 0.9026\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9118\n",
      "Epoch 00603: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9118\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8817\n",
      "Epoch 00604: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.8817\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.8605\n",
      "Epoch 00605: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8605\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8827\n",
      "Epoch 00606: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8827\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8922\n",
      "Epoch 00607: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3371 - accuracy: 0.8922\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9411\n",
      "Epoch 00608: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1699 - accuracy: 0.9411\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.9294\n",
      "Epoch 00609: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9294\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8944\n",
      "Epoch 00610: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2927 - accuracy: 0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.8381\n",
      "Epoch 00611: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8381\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9082\n",
      "Epoch 00612: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.9082\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2084 - accuracy: 0.9479\n",
      "Epoch 00613: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2084 - accuracy: 0.9479\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9648\n",
      "Epoch 00614: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9648\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.8037\n",
      "Epoch 00615: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4554 - accuracy: 0.8037\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9080\n",
      "Epoch 00616: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2189 - accuracy: 0.9080\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8507\n",
      "Epoch 00617: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3417 - accuracy: 0.8507\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8698\n",
      "Epoch 00618: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8698\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8101\n",
      "Epoch 00619: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4075 - accuracy: 0.8101\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9587\n",
      "Epoch 00620: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9587\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.8897\n",
      "Epoch 00621: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8897\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.8580\n",
      "Epoch 00622: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3086 - accuracy: 0.8580\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9076\n",
      "Epoch 00623: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.9076\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.8953\n",
      "Epoch 00624: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8953\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8844\n",
      "Epoch 00625: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8844\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9411\n",
      "Epoch 00626: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9411\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9189\n",
      "Epoch 00627: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9189\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9596\n",
      "Epoch 00628: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1913 - accuracy: 0.9596\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.9128\n",
      "Epoch 00629: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.9128\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9118\n",
      "Epoch 00630: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9118\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9517\n",
      "Epoch 00631: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9517\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.8464\n",
      "Epoch 00632: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4492 - accuracy: 0.8464\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.7968\n",
      "Epoch 00633: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7968\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9423\n",
      "Epoch 00634: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1756 - accuracy: 0.9423\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.8620\n",
      "Epoch 00635: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3615 - accuracy: 0.8620\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8971\n",
      "Epoch 00636: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8971\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8613\n",
      "Epoch 00637: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.8613\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.9158\n",
      "Epoch 00638: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.9158\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8557\n",
      "Epoch 00639: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3775 - accuracy: 0.8557\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9655\n",
      "Epoch 00640: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9655\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9216\n",
      "Epoch 00641: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2610 - accuracy: 0.9216\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.9087\n",
      "Epoch 00642: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.9087\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9356\n",
      "Epoch 00643: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9356\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9192\n",
      "Epoch 00644: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.9192\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8750\n",
      "Epoch 00645: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3848 - accuracy: 0.8750\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9275\n",
      "Epoch 00646: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 647/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9127\n",
      "Epoch 00647: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2692 - accuracy: 0.9127\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8714\n",
      "Epoch 00648: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2665 - accuracy: 0.8714\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9005\n",
      "Epoch 00649: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2590 - accuracy: 0.9005\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8956\n",
      "Epoch 00650: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8956\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9550\n",
      "Epoch 00651: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9550\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.8902\n",
      "Epoch 00652: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3470 - accuracy: 0.8902\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8777\n",
      "Epoch 00653: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8777\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.8999\n",
      "Epoch 00654: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.8999\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8437\n",
      "Epoch 00655: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8437\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8253\n",
      "Epoch 00656: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8253\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4251 - accuracy: 0.8170\n",
      "Epoch 00657: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8170\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9627\n",
      "Epoch 00658: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9627\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9251\n",
      "Epoch 00659: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.9251\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9659\n",
      "Epoch 00660: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1823 - accuracy: 0.9659\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8452\n",
      "Epoch 00661: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8452\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9191\n",
      "Epoch 00662: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9191\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.8938\n",
      "Epoch 00663: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8938\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.9229\n",
      "Epoch 00664: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1914 - accuracy: 0.9229\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9507\n",
      "Epoch 00665: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9507\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8258\n",
      "Epoch 00666: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.8258\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9525\n",
      "Epoch 00667: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1596 - accuracy: 0.9525\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8980\n",
      "Epoch 00668: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3004 - accuracy: 0.8980\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.9125\n",
      "Epoch 00669: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.9125\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8621\n",
      "Epoch 00670: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3296 - accuracy: 0.8621\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9450\n",
      "Epoch 00671: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9450\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9328\n",
      "Epoch 00672: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9328\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3902 - accuracy: 0.8190\n",
      "Epoch 00673: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8190\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9222\n",
      "Epoch 00674: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2343 - accuracy: 0.9222\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8949\n",
      "Epoch 00675: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8949\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9664\n",
      "Epoch 00676: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9664\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9494\n",
      "Epoch 00677: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9494\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.8916\n",
      "Epoch 00678: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8916\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8475\n",
      "Epoch 00679: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3738 - accuracy: 0.8475\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9354\n",
      "Epoch 00680: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9354\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9325\n",
      "Epoch 00681: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2121 - accuracy: 0.9325\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9074\n",
      "Epoch 00682: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 683/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8268\n",
      "Epoch 00683: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8268\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9061\n",
      "Epoch 00684: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2322 - accuracy: 0.9061\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9113\n",
      "Epoch 00685: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.9113\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3630 - accuracy: 0.8726\n",
      "Epoch 00686: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3630 - accuracy: 0.8726\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.7905\n",
      "Epoch 00687: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4331 - accuracy: 0.7905\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9120\n",
      "Epoch 00688: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.9120\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.8959\n",
      "Epoch 00689: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.8959\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9211\n",
      "Epoch 00690: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.9211\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.8975\n",
      "Epoch 00691: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.8975\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9199\n",
      "Epoch 00692: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2010 - accuracy: 0.9199\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.8577\n",
      "Epoch 00693: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8577\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8593\n",
      "Epoch 00694: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8593\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8538\n",
      "Epoch 00695: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3731 - accuracy: 0.8538\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9181\n",
      "Epoch 00696: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9181\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9230\n",
      "Epoch 00697: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.9230\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9190\n",
      "Epoch 00698: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.9190\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9530\n",
      "Epoch 00699: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9530\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.8739\n",
      "Epoch 00700: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8739\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2636 - accuracy: 0.9240\n",
      "Epoch 00701: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.9240\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9029\n",
      "Epoch 00702: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2493 - accuracy: 0.9029\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9091\n",
      "Epoch 00703: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2863 - accuracy: 0.9091\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8885\n",
      "Epoch 00704: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8885\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.8896\n",
      "Epoch 00705: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2446 - accuracy: 0.8896\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3964 - accuracy: 0.8236\n",
      "Epoch 00706: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3964 - accuracy: 0.8236\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8334\n",
      "Epoch 00707: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3257 - accuracy: 0.8334\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9400\n",
      "Epoch 00708: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9400\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.9294\n",
      "Epoch 00709: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.9294\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9169\n",
      "Epoch 00710: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2131 - accuracy: 0.9169\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9375\n",
      "Epoch 00711: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9375\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9238\n",
      "Epoch 00712: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2149 - accuracy: 0.9238\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8776\n",
      "Epoch 00713: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2913 - accuracy: 0.8776\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.9267\n",
      "Epoch 00714: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9267\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8277\n",
      "Epoch 00715: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4440 - accuracy: 0.8277\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3781 - accuracy: 0.8707\n",
      "Epoch 00716: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3781 - accuracy: 0.8707\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9501\n",
      "Epoch 00717: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1923 - accuracy: 0.9501\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9152\n",
      "Epoch 00718: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2691 - accuracy: 0.9152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 719/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8831\n",
      "Epoch 00719: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2665 - accuracy: 0.8831\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9160\n",
      "Epoch 00720: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.9160\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8466\n",
      "Epoch 00721: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8466\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1605 - accuracy: 0.9577\n",
      "Epoch 00722: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1605 - accuracy: 0.9577\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9673\n",
      "Epoch 00723: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1719 - accuracy: 0.9673\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9377\n",
      "Epoch 00724: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9377\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3188 - accuracy: 0.8937\n",
      "Epoch 00725: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8937\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8469\n",
      "Epoch 00726: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3900 - accuracy: 0.8469\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9234\n",
      "Epoch 00727: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9234\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.8902\n",
      "Epoch 00728: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2553 - accuracy: 0.8902\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8587\n",
      "Epoch 00729: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3459 - accuracy: 0.8587\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8386\n",
      "Epoch 00730: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3624 - accuracy: 0.8386\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9483\n",
      "Epoch 00731: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9483\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8425\n",
      "Epoch 00732: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3283 - accuracy: 0.8425\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9026\n",
      "Epoch 00733: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3051 - accuracy: 0.9026\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9462\n",
      "Epoch 00734: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.9462\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8779\n",
      "Epoch 00735: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8779\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9620\n",
      "Epoch 00736: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2100 - accuracy: 0.9620\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8549\n",
      "Epoch 00737: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3037 - accuracy: 0.8549\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.8850\n",
      "Epoch 00738: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8850\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8220\n",
      "Epoch 00739: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8220\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9650\n",
      "Epoch 00740: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9650\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9117\n",
      "Epoch 00741: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2045 - accuracy: 0.9117\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9143\n",
      "Epoch 00742: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9143\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9295\n",
      "Epoch 00743: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9295\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8539\n",
      "Epoch 00744: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3410 - accuracy: 0.8539\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.8990\n",
      "Epoch 00745: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2293 - accuracy: 0.8990\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.9074\n",
      "Epoch 00746: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2740 - accuracy: 0.9074\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9285\n",
      "Epoch 00747: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9285\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8464\n",
      "Epoch 00748: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8464\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9226\n",
      "Epoch 00749: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.9226\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8966\n",
      "Epoch 00750: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3108 - accuracy: 0.8966\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9189\n",
      "Epoch 00751: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2585 - accuracy: 0.9189\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9670\n",
      "Epoch 00752: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2279 - accuracy: 0.9670\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9193\n",
      "Epoch 00753: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.9193\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9074\n",
      "Epoch 00754: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2481 - accuracy: 0.9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.9156\n",
      "Epoch 00755: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3052 - accuracy: 0.9156\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8928\n",
      "Epoch 00756: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2667 - accuracy: 0.8928\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9148\n",
      "Epoch 00757: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9148\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5978 - accuracy: 0.7308\n",
      "Epoch 00758: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5978 - accuracy: 0.7308\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8507\n",
      "Epoch 00759: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8507\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.8590\n",
      "Epoch 00760: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3393 - accuracy: 0.8590\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9401\n",
      "Epoch 00761: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2061 - accuracy: 0.9401\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9164\n",
      "Epoch 00762: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3127 - accuracy: 0.9164\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9345\n",
      "Epoch 00763: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2461 - accuracy: 0.9345\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8736\n",
      "Epoch 00764: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2700 - accuracy: 0.8736\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9385\n",
      "Epoch 00765: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9385\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.7974\n",
      "Epoch 00766: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4826 - accuracy: 0.7974\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9475\n",
      "Epoch 00767: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9475\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9506\n",
      "Epoch 00768: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1892 - accuracy: 0.9506\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9373\n",
      "Epoch 00769: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9373\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3937 - accuracy: 0.8472\n",
      "Epoch 00770: loss did not improve from 0.14056\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8472\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9646\n",
      "Epoch 00771: loss improved from 0.14056 to 0.13414, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.1341 - accuracy: 0.9646\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8730\n",
      "Epoch 00772: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3353 - accuracy: 0.8730\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8232\n",
      "Epoch 00773: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3318 - accuracy: 0.8232\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8984\n",
      "Epoch 00774: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.8984\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8687\n",
      "Epoch 00775: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.8687\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9393\n",
      "Epoch 00776: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2374 - accuracy: 0.9393\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.8791\n",
      "Epoch 00777: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.8791\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9635\n",
      "Epoch 00778: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9635\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.8840\n",
      "Epoch 00779: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2869 - accuracy: 0.8840\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9031\n",
      "Epoch 00780: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2574 - accuracy: 0.9031\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8417\n",
      "Epoch 00781: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8417\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9147\n",
      "Epoch 00782: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1700 - accuracy: 0.9147\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3932 - accuracy: 0.8283\n",
      "Epoch 00783: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3932 - accuracy: 0.8283\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.8998\n",
      "Epoch 00784: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8998\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9575\n",
      "Epoch 00785: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9575\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9109\n",
      "Epoch 00786: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.9109\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8590\n",
      "Epoch 00787: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3100 - accuracy: 0.8590\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9078\n",
      "Epoch 00788: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9078\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9340\n",
      "Epoch 00789: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.9340\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.8959\n",
      "Epoch 00790: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.8959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9290\n",
      "Epoch 00791: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9290\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9369\n",
      "Epoch 00792: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2032 - accuracy: 0.9369\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.8789\n",
      "Epoch 00793: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8789\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8717\n",
      "Epoch 00794: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2919 - accuracy: 0.8717\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8960\n",
      "Epoch 00795: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.8960\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.8543\n",
      "Epoch 00796: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8543\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.9031\n",
      "Epoch 00797: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3006 - accuracy: 0.9031\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.8014\n",
      "Epoch 00798: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.8014\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9294\n",
      "Epoch 00799: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2509 - accuracy: 0.9294\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.9453\n",
      "Epoch 00800: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9453\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9661\n",
      "Epoch 00801: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9661\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9642\n",
      "Epoch 00802: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2074 - accuracy: 0.9642\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.8298\n",
      "Epoch 00803: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3247 - accuracy: 0.8298\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9012\n",
      "Epoch 00804: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2865 - accuracy: 0.9012\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.8910\n",
      "Epoch 00805: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8910\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.9180\n",
      "Epoch 00806: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2540 - accuracy: 0.9180\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8103\n",
      "Epoch 00807: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.4232 - accuracy: 0.8103\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9100\n",
      "Epoch 00808: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9100\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.9017\n",
      "Epoch 00809: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3029 - accuracy: 0.9017\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9359\n",
      "Epoch 00810: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9359\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8412\n",
      "Epoch 00811: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3791 - accuracy: 0.8412\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.8888\n",
      "Epoch 00812: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.8888\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9003\n",
      "Epoch 00813: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9003\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9094\n",
      "Epoch 00814: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9094\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9663\n",
      "Epoch 00815: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9663\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8897\n",
      "Epoch 00816: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8897\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9063\n",
      "Epoch 00817: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2806 - accuracy: 0.9063\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.8487\n",
      "Epoch 00818: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8487\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9339\n",
      "Epoch 00819: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9339\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9220\n",
      "Epoch 00820: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2523 - accuracy: 0.9220\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8419\n",
      "Epoch 00821: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8419\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9353\n",
      "Epoch 00822: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9353\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9075\n",
      "Epoch 00823: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9075\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.8690\n",
      "Epoch 00824: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8690\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9112\n",
      "Epoch 00825: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2465 - accuracy: 0.9112\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9604\n",
      "Epoch 00826: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1672 - accuracy: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9194\n",
      "Epoch 00827: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9194\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8543\n",
      "Epoch 00828: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8543\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.9097\n",
      "Epoch 00829: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.9097\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9503\n",
      "Epoch 00830: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9503\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9377\n",
      "Epoch 00831: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9377\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3290 - accuracy: 0.8726\n",
      "Epoch 00832: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8726\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.9408\n",
      "Epoch 00833: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1877 - accuracy: 0.9408\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9022\n",
      "Epoch 00834: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2276 - accuracy: 0.9022\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.7900\n",
      "Epoch 00835: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7900\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9057\n",
      "Epoch 00836: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2213 - accuracy: 0.9057\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.8535\n",
      "Epoch 00837: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8535\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9440\n",
      "Epoch 00838: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2594 - accuracy: 0.9440\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8113\n",
      "Epoch 00839: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8113\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.9198\n",
      "Epoch 00840: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2402 - accuracy: 0.9198\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.8915\n",
      "Epoch 00841: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8915\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9174\n",
      "Epoch 00842: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9174\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9185\n",
      "Epoch 00843: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.9185\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9019\n",
      "Epoch 00844: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2542 - accuracy: 0.9019\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9394\n",
      "Epoch 00845: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1767 - accuracy: 0.9394\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9460\n",
      "Epoch 00846: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9460\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9289\n",
      "Epoch 00847: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.9289\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9600\n",
      "Epoch 00848: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1501 - accuracy: 0.9600\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9663\n",
      "Epoch 00849: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1351 - accuracy: 0.9663\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.7988\n",
      "Epoch 00850: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3851 - accuracy: 0.7988\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8553\n",
      "Epoch 00851: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3788 - accuracy: 0.8553\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9313\n",
      "Epoch 00852: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9313\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.7959\n",
      "Epoch 00853: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.7959\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8359\n",
      "Epoch 00854: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8359\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.8879\n",
      "Epoch 00855: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8879\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9549\n",
      "Epoch 00856: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9549\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.8466\n",
      "Epoch 00857: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3125 - accuracy: 0.8466\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.8730\n",
      "Epoch 00858: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.8730\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9284\n",
      "Epoch 00859: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2347 - accuracy: 0.9284\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9127\n",
      "Epoch 00860: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9127\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8768\n",
      "Epoch 00861: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8768\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9379\n",
      "Epoch 00862: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 863/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8417\n",
      "Epoch 00863: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.4204 - accuracy: 0.8417\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.8573\n",
      "Epoch 00864: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2666 - accuracy: 0.8573\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9230\n",
      "Epoch 00865: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9230\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8723\n",
      "Epoch 00866: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8723\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8281\n",
      "Epoch 00867: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8281\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9664\n",
      "Epoch 00868: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9664\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9124\n",
      "Epoch 00869: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9124\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9216\n",
      "Epoch 00870: loss did not improve from 0.13414\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.9216\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9566\n",
      "Epoch 00871: loss improved from 0.13414 to 0.11746, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.1175 - accuracy: 0.9566\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9340\n",
      "Epoch 00872: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9340\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7830\n",
      "Epoch 00873: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5032 - accuracy: 0.7830\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.8983\n",
      "Epoch 00874: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8983\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9552\n",
      "Epoch 00875: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1372 - accuracy: 0.9552\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.8966\n",
      "Epoch 00876: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8966\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9592\n",
      "Epoch 00877: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1955 - accuracy: 0.9592\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9538\n",
      "Epoch 00878: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2331 - accuracy: 0.9538\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9370\n",
      "Epoch 00879: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.9370\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8305\n",
      "Epoch 00880: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8305\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.8918\n",
      "Epoch 00881: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2425 - accuracy: 0.8918\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8520\n",
      "Epoch 00882: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.8520\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8835\n",
      "Epoch 00883: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2929 - accuracy: 0.8835\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8315\n",
      "Epoch 00884: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8315\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.8930\n",
      "Epoch 00885: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3203 - accuracy: 0.8930\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9344\n",
      "Epoch 00886: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9344\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9321\n",
      "Epoch 00887: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2229 - accuracy: 0.9321\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9002\n",
      "Epoch 00888: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1779 - accuracy: 0.9002\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9310\n",
      "Epoch 00889: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1796 - accuracy: 0.9310\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8998\n",
      "Epoch 00890: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2861 - accuracy: 0.8998\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8215\n",
      "Epoch 00891: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3478 - accuracy: 0.8215\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9213\n",
      "Epoch 00892: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1995 - accuracy: 0.9213\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9339\n",
      "Epoch 00893: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1809 - accuracy: 0.9339\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.9132\n",
      "Epoch 00894: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2434 - accuracy: 0.9132\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8625\n",
      "Epoch 00895: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2986 - accuracy: 0.8625\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.7935\n",
      "Epoch 00896: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.7935\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8172\n",
      "Epoch 00897: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3845 - accuracy: 0.8172\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8780\n",
      "Epoch 00898: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9642\n",
      "Epoch 00899: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9642\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9506\n",
      "Epoch 00900: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1865 - accuracy: 0.9506\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.8839\n",
      "Epoch 00901: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2471 - accuracy: 0.8839\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8887\n",
      "Epoch 00902: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3824 - accuracy: 0.8887\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9339\n",
      "Epoch 00903: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9339\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9643\n",
      "Epoch 00904: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9643\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8417\n",
      "Epoch 00905: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3099 - accuracy: 0.8417\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.8729\n",
      "Epoch 00906: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.8729\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9319\n",
      "Epoch 00907: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9319\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.8785\n",
      "Epoch 00908: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8785\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9453\n",
      "Epoch 00909: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9453\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9152\n",
      "Epoch 00910: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9152\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9347\n",
      "Epoch 00911: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2009 - accuracy: 0.9347\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8518\n",
      "Epoch 00912: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3440 - accuracy: 0.8518\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9388\n",
      "Epoch 00913: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1749 - accuracy: 0.9388\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.8030\n",
      "Epoch 00914: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4624 - accuracy: 0.8030\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.9071\n",
      "Epoch 00915: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.9071\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9005\n",
      "Epoch 00916: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2421 - accuracy: 0.9005\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9404\n",
      "Epoch 00917: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1838 - accuracy: 0.9404\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3592 - accuracy: 0.8223\n",
      "Epoch 00918: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.8223\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.7807\n",
      "Epoch 00919: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4173 - accuracy: 0.7807\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9230\n",
      "Epoch 00920: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2195 - accuracy: 0.9230\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8758\n",
      "Epoch 00921: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3351 - accuracy: 0.8758\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9616\n",
      "Epoch 00922: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9616\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.8297\n",
      "Epoch 00923: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2972 - accuracy: 0.8297\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.8914\n",
      "Epoch 00924: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2189 - accuracy: 0.8914\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9207\n",
      "Epoch 00925: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2514 - accuracy: 0.9207\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9183\n",
      "Epoch 00926: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9183\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9133\n",
      "Epoch 00927: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2534 - accuracy: 0.9133\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9543\n",
      "Epoch 00928: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1823 - accuracy: 0.9543\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.8942\n",
      "Epoch 00929: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2543 - accuracy: 0.8942\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9339\n",
      "Epoch 00930: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2122 - accuracy: 0.9339\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.8599\n",
      "Epoch 00931: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8599\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.8903\n",
      "Epoch 00932: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2590 - accuracy: 0.8903\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8217\n",
      "Epoch 00933: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8217\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9182\n",
      "Epoch 00934: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.9182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 935/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9128\n",
      "Epoch 00935: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2138 - accuracy: 0.9128\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.8543\n",
      "Epoch 00936: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8543\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8311\n",
      "Epoch 00937: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8311\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9128\n",
      "Epoch 00938: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.9128\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9443\n",
      "Epoch 00939: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1927 - accuracy: 0.9443\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.8883\n",
      "Epoch 00940: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2392 - accuracy: 0.8883\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9392\n",
      "Epoch 00941: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.1560 - accuracy: 0.9392\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9322\n",
      "Epoch 00942: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9322\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9108\n",
      "Epoch 00943: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.9108\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9441\n",
      "Epoch 00944: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1975 - accuracy: 0.9441\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9058\n",
      "Epoch 00945: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1932 - accuracy: 0.9058\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8599\n",
      "Epoch 00946: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8599\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9243\n",
      "Epoch 00947: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2043 - accuracy: 0.9243\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.9189\n",
      "Epoch 00948: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9189\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.7885\n",
      "Epoch 00949: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3427 - accuracy: 0.7885\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9405\n",
      "Epoch 00950: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9405\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9193\n",
      "Epoch 00951: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.9193\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.8734\n",
      "Epoch 00952: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8734\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9237\n",
      "Epoch 00953: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2565 - accuracy: 0.9237\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9025\n",
      "Epoch 00954: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1542 - accuracy: 0.9025\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9220\n",
      "Epoch 00955: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9220\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9169\n",
      "Epoch 00956: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9169\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9314\n",
      "Epoch 00957: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9314\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.8392\n",
      "Epoch 00958: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.8392\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9527\n",
      "Epoch 00959: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9527\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.8673\n",
      "Epoch 00960: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2529 - accuracy: 0.8673\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8504\n",
      "Epoch 00961: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8504\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9654\n",
      "Epoch 00962: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9654\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.8755\n",
      "Epoch 00963: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3156 - accuracy: 0.8755\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.8087\n",
      "Epoch 00964: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8087\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9335\n",
      "Epoch 00965: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1451 - accuracy: 0.9335\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9447\n",
      "Epoch 00966: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1724 - accuracy: 0.9447\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3487 - accuracy: 0.9086\n",
      "Epoch 00967: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.9086\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9188\n",
      "Epoch 00968: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9188\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.8770\n",
      "Epoch 00969: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2947 - accuracy: 0.8770\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9636\n",
      "Epoch 00970: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1208 - accuracy: 0.9636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 971/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.7795\n",
      "Epoch 00971: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.7795\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9475\n",
      "Epoch 00972: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1578 - accuracy: 0.9475\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.8740\n",
      "Epoch 00973: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8740\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8793\n",
      "Epoch 00974: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3343 - accuracy: 0.8793\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9244\n",
      "Epoch 00975: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.9244\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1609 - accuracy: 0.9487\n",
      "Epoch 00976: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9487\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9298\n",
      "Epoch 00977: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1347 - accuracy: 0.9298\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.8410\n",
      "Epoch 00978: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.8410\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8455\n",
      "Epoch 00979: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3474 - accuracy: 0.8455\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.8510\n",
      "Epoch 00980: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8510\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9147\n",
      "Epoch 00981: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2327 - accuracy: 0.9147\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9125\n",
      "Epoch 00982: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.9125\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8422\n",
      "Epoch 00983: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.8422\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9140\n",
      "Epoch 00984: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9140\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8369\n",
      "Epoch 00985: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3229 - accuracy: 0.8369\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.8885\n",
      "Epoch 00986: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.8885\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8852\n",
      "Epoch 00987: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8852\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9382\n",
      "Epoch 00988: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9382\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9646\n",
      "Epoch 00989: loss did not improve from 0.11746\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9646\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9687\n",
      "Epoch 00990: loss improved from 0.11746 to 0.09903, saving model to unet_membrane.hdf5\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0990 - accuracy: 0.9687\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9266\n",
      "Epoch 00991: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9266\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9331\n",
      "Epoch 00992: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9331\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9505\n",
      "Epoch 00993: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9505\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9455\n",
      "Epoch 00994: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9455\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9005\n",
      "Epoch 00995: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2363 - accuracy: 0.9005\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8346\n",
      "Epoch 00996: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.3097 - accuracy: 0.8346\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.8308\n",
      "Epoch 00997: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2997 - accuracy: 0.8308\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.8983\n",
      "Epoch 00998: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.8983\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9034\n",
      "Epoch 00999: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9034\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8726\n",
      "Epoch 01000: loss did not improve from 0.09903\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8726\n"
     ]
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(2,'data/train','image','label',data_gen_args,save_to_dir = None)\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "history = model.fit_generator(myGene,steps_per_epoch=1,epochs=1000,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "train_loss = history.history['loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "# Times New Roman\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, train_accuracy, 'r', label='Training Accuracy')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# \n",
    "plt.savefig('training_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs_train,imgs_mask_train = geneTrainNpy(\"data/membrane/train/aug/\",\"data/membrane/train/aug/\")\n",
    "#model.fit(imgs_train, imgs_mask_train, batch_size=2, nb_epoch=10, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test your model and save predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020E23DC5C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 174ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "testGene = testGenerator(\"data/test\")\n",
    "model = unet()\n",
    "model.load_weights(\"unet_membrane.hdf5\")\n",
    "results = model.predict_generator(testGene,3,verbose=1)\n",
    "saveResult(\"data/test\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
